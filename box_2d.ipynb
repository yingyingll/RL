{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple\n",
      "Requirement already satisfied: paddlepaddle==1.6.3 in /anaconda3/lib/python3.7/site-packages (1.6.3)\n",
      "Requirement already satisfied: graphviz in /anaconda3/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (0.14)\n",
      "Requirement already satisfied: requests>=2.20.0 in /anaconda3/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (2.23.0)\n",
      "Requirement already satisfied: scipy; python_version >= \"3.5\" in /anaconda3/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (1.1.0)\n",
      "Requirement already satisfied: objgraph in /anaconda3/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (3.4.1)\n",
      "Requirement already satisfied: pyyaml in /anaconda3/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (5.3)\n",
      "Requirement already satisfied: matplotlib; python_version >= \"3.6\" in /anaconda3/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (2.2.3)\n",
      "Requirement already satisfied: funcsigs in /anaconda3/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (1.0.2)\n",
      "Requirement already satisfied: prettytable in /anaconda3/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (0.7.2)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (1.14.0)\n",
      "Requirement already satisfied: Pillow in /anaconda3/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (5.4.1)\n",
      "Requirement already satisfied: opencv-python in /anaconda3/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (4.1.1.26)\n",
      "Requirement already satisfied: numpy>=1.12; python_version >= \"3.5\" in /anaconda3/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (1.17.2)\n",
      "Requirement already satisfied: rarfile in /anaconda3/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (3.1)\n",
      "Requirement already satisfied: decorator in /anaconda3/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (4.4.2)\n",
      "Requirement already satisfied: nltk; python_version >= \"3.5\" in /anaconda3/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (3.4.5)\n",
      "Requirement already satisfied: protobuf>=3.1.0 in /anaconda3/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (3.9.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle==1.6.3) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /anaconda3/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle==1.6.3) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /anaconda3/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle==1.6.3) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /anaconda3/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle==1.6.3) (2.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /anaconda3/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->paddlepaddle==1.6.3) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /anaconda3/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->paddlepaddle==1.6.3) (2.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /anaconda3/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->paddlepaddle==1.6.3) (2.8.1)\n",
      "Requirement already satisfied: pytz in /anaconda3/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->paddlepaddle==1.6.3) (2019.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /anaconda3/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->paddlepaddle==1.6.3) (1.1.0)\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.7/site-packages (from protobuf>=3.1.0->paddlepaddle==1.6.3) (46.0.0.post20200309)\n",
      "Looking in indexes: https://mirror.baidu.com/pypi/simple\n",
      "Requirement already satisfied: parl==1.3.1 in /anaconda3/lib/python3.7/site-packages (1.3.1)\n",
      "Requirement already satisfied: psutil>=5.6.2 in /anaconda3/lib/python3.7/site-packages (from parl==1.3.1) (5.7.0)\n",
      "Requirement already satisfied: tensorboardX==1.8 in /anaconda3/lib/python3.7/site-packages (from parl==1.3.1) (1.8)\n",
      "Requirement already satisfied: flask>=1.0.4 in /anaconda3/lib/python3.7/site-packages (from parl==1.3.1) (1.1.1)\n",
      "Requirement already satisfied: pyarrow==0.13.0 in /anaconda3/lib/python3.7/site-packages (from parl==1.3.1) (0.13.0)\n",
      "Requirement already satisfied: tb-nightly==1.15.0a20190801 in /anaconda3/lib/python3.7/site-packages (from parl==1.3.1) (1.15.0a20190801)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /anaconda3/lib/python3.7/site-packages (from parl==1.3.1) (1.1.0)\n",
      "Requirement already satisfied: click in /anaconda3/lib/python3.7/site-packages (from parl==1.3.1) (7.1.1)\n",
      "Requirement already satisfied: pyzmq==18.0.1 in /anaconda3/lib/python3.7/site-packages (from parl==1.3.1) (18.0.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /anaconda3/lib/python3.7/site-packages (from parl==1.3.1) (1.1.0)\n",
      "Requirement already satisfied: cloudpickle==1.2.1 in /anaconda3/lib/python3.7/site-packages (from parl==1.3.1) (1.2.1)\n",
      "Requirement already satisfied: flask-cors in /anaconda3/lib/python3.7/site-packages (from parl==1.3.1) (3.0.8)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.7/site-packages (from tensorboardX==1.8->parl==1.3.1) (1.14.0)\n",
      "Requirement already satisfied: numpy in /anaconda3/lib/python3.7/site-packages (from tensorboardX==1.8->parl==1.3.1) (1.17.2)\n",
      "Requirement already satisfied: protobuf>=3.2.0 in /anaconda3/lib/python3.7/site-packages (from tensorboardX==1.8->parl==1.3.1) (3.9.1)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /anaconda3/lib/python3.7/site-packages (from flask>=1.0.4->parl==1.3.1) (1.0.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /anaconda3/lib/python3.7/site-packages (from flask>=1.0.4->parl==1.3.1) (2.11.1)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /anaconda3/lib/python3.7/site-packages (from flask>=1.0.4->parl==1.3.1) (1.1.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /anaconda3/lib/python3.7/site-packages (from tb-nightly==1.15.0a20190801->parl==1.3.1) (0.7.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /anaconda3/lib/python3.7/site-packages (from tb-nightly==1.15.0a20190801->parl==1.3.1) (46.0.0.post20200309)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /anaconda3/lib/python3.7/site-packages (from tb-nightly==1.15.0a20190801->parl==1.3.1) (0.34.2)\n",
      "Requirement already satisfied: grpcio>=1.6.3 in /anaconda3/lib/python3.7/site-packages (from tb-nightly==1.15.0a20190801->parl==1.3.1) (1.23.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /anaconda3/lib/python3.7/site-packages (from tb-nightly==1.15.0a20190801->parl==1.3.1) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /anaconda3/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask>=1.0.4->parl==1.3.1) (1.1.1)\n",
      "Looking in indexes: https://mirror.baidu.com/pypi/simple\n",
      "Collecting rlschool==0.3.1\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/45/8b/a6885b8815e364f21a37e0580d1018871a8aab882586e87d60f1f6a1e55a/rlschool-0.3.1-py3-none-any.whl (461 kB)\n",
      "\u001b[K     |████████████████████████████████| 461 kB 5.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Pillow>=6.2.2\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/98/83/0fdb0910c909f40c090ba09184feb59001b7fcc89c676fb77986a262af85/Pillow-7.1.2-cp37-cp37m-macosx_10_10_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 18.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /anaconda3/lib/python3.7/site-packages (from rlschool==0.3.1) (1.14.0)\n",
      "Collecting trimesh>=3.2.39\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/8f/81/4d65f05e9e401bb30c880cd99e3c55cc02c05c80b172af763dd3be9b2ae9/trimesh-3.7.0-py3-none-any.whl (614 kB)\n",
      "\u001b[K     |████████████████████████████████| 614 kB 15.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: networkx>=2.2 in /anaconda3/lib/python3.7/site-packages (from rlschool==0.3.1) (2.4)\n",
      "Collecting pyglet==1.5.0; python_version >= \"3\"\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/70/ca/20aee170afe6011e295e34b27ad7d7ccd795faba581dd3c6f7cec237f561/pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 11.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting colour>=0.1.5\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/74/46/e81907704ab203206769dee1385dc77e1407576ff8f50a0681d0a6b541be/colour-0.1.5-py2.py3-none-any.whl (23 kB)\n",
      "Collecting configparser>=3.7.4\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: numpy>=1.16.4 in /anaconda3/lib/python3.7/site-packages (from rlschool==0.3.1) (1.17.2)\n",
      "Requirement already satisfied: scipy>=0.12.0 in /anaconda3/lib/python3.7/site-packages (from rlschool==0.3.1) (1.1.0)\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.7/site-packages (from trimesh>=3.2.39->rlschool==0.3.1) (46.0.0.post20200309)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /anaconda3/lib/python3.7/site-packages (from networkx>=2.2->rlschool==0.3.1) (4.4.2)\n",
      "Requirement already satisfied: future in /anaconda3/lib/python3.7/site-packages (from pyglet==1.5.0; python_version >= \"3\"->rlschool==0.3.1) (0.17.1)\n",
      "\u001b[31mERROR: gym 0.14.0 has requirement pyglet<=1.3.2,>=1.2.0, but you'll have pyglet 1.5.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: Pillow, trimesh, pyglet, colour, configparser, rlschool\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: Pillow 5.4.1\n",
      "    Uninstalling Pillow-5.4.1:\n",
      "      Successfully uninstalled Pillow-5.4.1\n",
      "  Attempting uninstall: pyglet\n",
      "    Found existing installation: pyglet 1.3.2\n",
      "    Uninstalling pyglet-1.3.2:\n",
      "      Successfully uninstalled pyglet-1.3.2\n",
      "Successfully installed Pillow-7.1.2 colour-0.1.5 configparser-5.0.0 pyglet-1.5.0 rlschool-0.3.1 trimesh-3.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install paddlepaddle==1.6.3 -i https://mirror.baidu.com/pypi/simple\n",
    "!pip install parl==1.3.1 -i https://mirror.baidu.com/pypi/simple\n",
    "!pip install rlschool==0.3.1 -i https://mirror.baidu.com/pypi/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paddlepaddle                       1.6.3              \n",
      "parl                               1.3.1              \n",
      "rlschool                           0.3.1              \n"
     ]
    }
   ],
   "source": [
    "# 检查依赖包版本是否正确\n",
    "!pip list | grep paddlepaddle\n",
    "!pip list | grep parl\n",
    "!pip list | grep rlschool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.7/site-packages/pandas/core/tools/datetimes.py:3: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import MutableMapping\n",
      "/anaconda3/lib/python3.7/site-packages/matplotlib/rcsetup.py:20: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable, Mapping\n",
      "/anaconda3/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sized\n",
      "/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import parl\n",
    "from parl import layers\n",
    "from paddle import fluid\n",
    "from parl.utils import logger\n",
    "from parl.utils import action_mapping # 将神经网络输出映射到对应的 实际动作取值范围 内\n",
    "from parl.utils import ReplayMemory # 经验回放\n",
    "\n",
    "from rlschool import make_env  # 使用 RLSchool 创建飞行器环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "######################################################################\n",
    "#\n",
    "# 1. 请设定 learning rate，尝试增减查看效果\n",
    "#\n",
    "######################################################################\n",
    "######################################################################\n",
    "ACTOR_LR = 0.0002   # Actor网络更新的 learning rate\n",
    "CRITIC_LR = 0.001   # Critic网络更新的 learning rate\n",
    "\n",
    "GAMMA = 0.99        # reward 的衰减因子，一般取 0.9 到 0.999 不等\n",
    "TAU = 0.001         # target_model 跟 model 同步参数 的 软更新参数\n",
    "MEMORY_SIZE = 1e6   # replay memory的大小，越大越占用内存\n",
    "MEMORY_WARMUP_SIZE = 1e4      # replay_memory 里需要预存一些经验数据，再从里面sample一个batch的经验让agent去learn\n",
    "REWARD_SCALE = 0.01       # reward 的缩放因子\n",
    "BATCH_SIZE = 256          # 每次给agent learn的数据数量，从replay memory随机里sample一批数据出来\n",
    "TRAIN_TOTAL_STEPS = 1e6   # 总训练步数\n",
    "TEST_EVERY_STEPS = 1e4    # 每个N步评估一下算法效果，每次评估5个episode求平均reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyModel(parl.Model):\n",
    "    def __init__(self, act_dim, init_logvar=-1):\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        #\n",
    "        # 2. 请配置model结构\n",
    "        #\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        self.act_dim = act_dim\n",
    "        self.fc1 = layers.fc(size=100, act='relu')\n",
    "        self.fc2 = layers.fc(size=act_dim, act='tanh')\n",
    "        \n",
    "        self.logvars = layers.create_parameter(\n",
    "            shape=[act_dim],\n",
    "            dtype='float32',\n",
    "            default_initializer=fluid.initializer.ConstantInitializer(\n",
    "                init_logvar))\n",
    "\n",
    "    def policy(self, obs):\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        #\n",
    "        # 3. 请组装policy网络\n",
    "        #\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        hid1 = self.fc1(obs)\n",
    "        means = self.fc2(hid1)\n",
    "        logvars = self.logvars()\n",
    "        return means, logvars\n",
    "\n",
    "    \n",
    "    def sample(self, obs):\n",
    "        means, logvars = self.policy(obs)\n",
    "        print(means, logvars)\n",
    "        print('aaaaa')\n",
    "        sampled_act = means + (\n",
    "            layers.exp(logvars / 2.0) *  # stddev\n",
    "            layers.gaussian_random(shape=(self.act_dim, ), dtype='float32'))\n",
    "        return sampled_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueModel(parl.Model):\n",
    "    def __init__(self):\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        #\n",
    "        # 4. 请配置model结构\n",
    "        #\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        self.fc1 = layers.fc(size=100, act='relu')\n",
    "        self.fc2 = layers.fc(size=1, act=None)\n",
    "\n",
    "class ValueModel(parl.Model):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super(ValueModel, self).__init__()\n",
    "        hid1_size = obs_dim * 10\n",
    "        hid3_size = 5\n",
    "        hid2_size = int(np.sqrt(hid1_size * hid3_size))\n",
    "\n",
    "        self.lr = 1e-2 / np.sqrt(hid2_size)\n",
    "\n",
    "        self.fc1 = layers.fc(size=hid1_size, act='tanh')\n",
    "        self.fc2 = layers.fc(size=1)\n",
    "\n",
    "    def value(self, obs):\n",
    "        hid1 = self.fc1(obs)\n",
    "        V = self.fc2(hid1)\n",
    "        V = layers.squeeze(V, axes=[])\n",
    "        return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class BipedalWalkerModel(parl.Model):\n",
    "    def __init__(self, obs_dim, act_dim, init_logvar=-1.0):\n",
    "        self.policy_model = PolicyModel(act_dim, init_logvar)\n",
    "        self.value_model = ValueModel(obs_dim, act_dim)\n",
    "#         self.policy_lr = self.policy_model.lr\n",
    "#         self.value_lr = self.value_model.lr\n",
    "\n",
    "    def policy(self, obs):\n",
    "        return self.policy_model.policy(obs)\n",
    "\n",
    "    def policy_sample(self, obs):\n",
    "        return self.policy_model.sample(obs)\n",
    "\n",
    "    def value(self, obs):\n",
    "        return self.value_model.value(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parl.algorithms import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiPedalwalkerAgent(parl.Agent):\n",
    "    def __init__(self,\n",
    "                 algorithm,\n",
    "                 obs_dim,\n",
    "                 act_dim,\n",
    "                 kl_targ,\n",
    "                 loss_type,\n",
    "                 beta=1.0,\n",
    "                 epsilon=0.2,\n",
    "                 policy_learn_times=20,\n",
    "                 value_learn_times=10,\n",
    "                 value_batch_size=256):\n",
    "        self.alg = algorithm\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        assert loss_type == 'CLIP' or loss_type == 'KLPEN'\n",
    "        self.loss_type = loss_type\n",
    "        super(BiPedalwalkerAgent, self).__init__(algorithm)\n",
    "\n",
    "        self.policy_learn_times = policy_learn_times\n",
    "        # Adaptive kl penalty coefficient\n",
    "        self.beta = beta\n",
    "        self.kl_targ = kl_targ\n",
    "\n",
    "        self.value_learn_times = value_learn_times\n",
    "        self.value_batch_size = value_batch_size\n",
    "        self.value_learn_buffer = None\n",
    "\n",
    "    def build_program(self):\n",
    "        self.policy_predict_program = fluid.Program()\n",
    "        self.policy_sample_program = fluid.Program()\n",
    "        self.policy_learn_program = fluid.Program()\n",
    "        self.value_predict_program = fluid.Program()\n",
    "        self.value_learn_program = fluid.Program()\n",
    "\n",
    "        with fluid.program_guard(self.policy_sample_program):\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            print(obs)\n",
    "            sampled_act = self.alg.sample(obs)\n",
    "            self.policy_sample_output = [sampled_act]\n",
    "\n",
    "        with fluid.program_guard(self.policy_predict_program):\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            means = self.alg.predict(obs)\n",
    "            self.policy_predict_output = [means]\n",
    "\n",
    "        with fluid.program_guard(self.policy_learn_program):\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            actions = layers.data(\n",
    "                name='actions', shape=[self.act_dim], dtype='float32')\n",
    "            advantages = layers.data(\n",
    "                name='advantages', shape=[1], dtype='float32')\n",
    "            if self.loss_type == 'KLPEN':\n",
    "                beta = layers.data(name='beta', shape=[], dtype='float32')\n",
    "                loss, kl = self.alg.policy_learn(obs, actions, advantages,\n",
    "                                                 beta)\n",
    "            else:\n",
    "                loss, kl = self.alg.policy_learn(obs, actions, advantages)\n",
    "\n",
    "            self.policy_learn_output = [loss, kl]\n",
    "\n",
    "        with fluid.program_guard(self.value_predict_program):\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            value = self.alg.value_predict(obs)\n",
    "            self.value_predict_output = [value]\n",
    "\n",
    "        with fluid.program_guard(self.value_learn_program):\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            val = layers.data(name='val', shape=[], dtype='float32')\n",
    "            value_loss = self.alg.value_learn(obs, val)\n",
    "            self.value_learn_output = [value_loss]\n",
    "\n",
    "    def policy_sample(self, obs):\n",
    "        feed = {'obs': obs}\n",
    "        sampled_act = self.fluid_executor.run(\n",
    "            self.policy_sample_program,\n",
    "            feed=feed,\n",
    "            fetch_list=self.policy_sample_output)[0]\n",
    "        return sampled_act\n",
    "\n",
    "    def policy_predict(self, obs):\n",
    "        feed = {'obs': obs}\n",
    "        means = self.fluid_executor.run(\n",
    "            self.policy_predict_program,\n",
    "            feed=feed,\n",
    "            fetch_list=self.policy_predict_output)[0]\n",
    "        return means\n",
    "\n",
    "    def value_predict(self, obs):\n",
    "        feed = {'obs': obs}\n",
    "        value = self.fluid_executor.run(\n",
    "            self.value_predict_program,\n",
    "            feed=feed,\n",
    "            fetch_list=self.value_predict_output)[0]\n",
    "        return value\n",
    "\n",
    "    def _batch_policy_learn(self, obs, actions, advantages):\n",
    "        if self.loss_type == 'KLPEN':\n",
    "            feed = {\n",
    "                'obs': obs,\n",
    "                'actions': actions,\n",
    "                'advantages': advantages,\n",
    "                'beta': self.beta\n",
    "            }\n",
    "        else:\n",
    "            feed = {'obs': obs, 'actions': actions, 'advantages': advantages}\n",
    "        [loss, kl] = self.fluid_executor.run(\n",
    "            self.policy_learn_program,\n",
    "            feed=feed,\n",
    "            fetch_list=self.policy_learn_output)\n",
    "        return loss, kl\n",
    "\n",
    "    def _batch_value_learn(self, obs, val):\n",
    "        feed = {'obs': obs, 'val': val}\n",
    "        value_loss = self.fluid_executor.run(\n",
    "            self.value_learn_program,\n",
    "            feed=feed,\n",
    "            fetch_list=self.value_learn_output)[0]\n",
    "        return value_loss\n",
    "\n",
    "    def policy_learn(self, obs, actions, advantages):\n",
    "        \"\"\" Learn policy:\n",
    "        1. Sync parameters of policy model to old policy model\n",
    "        2. Fix old policy model, and learn policy model multi times\n",
    "        3. if use KLPEN loss, Adjust kl loss coefficient: beta\n",
    "        \"\"\"\n",
    "        self.alg.sync_old_policy()\n",
    "\n",
    "        all_loss, all_kl = [], []\n",
    "        for _ in range(self.policy_learn_times):\n",
    "            loss, kl = self._batch_policy_learn(obs, actions, advantages)\n",
    "            all_loss.append(loss)\n",
    "            all_kl.append(kl)\n",
    "\n",
    "        if self.loss_type == 'KLPEN':\n",
    "            # Adative KL penalty coefficient\n",
    "            if kl > self.kl_targ * 2:\n",
    "                self.beta = 1.5 * self.beta\n",
    "            elif kl < self.kl_targ / 2:\n",
    "                self.beta = self.beta / 1.5\n",
    "\n",
    "        return np.mean(all_loss), np.mean(all_kl)\n",
    "\n",
    "    def value_learn(self, obs, value):\n",
    "        \"\"\" Fit model to current data batch + previous data batch\n",
    "        \"\"\"\n",
    "        data_size = obs.shape[0]\n",
    "\n",
    "        if self.value_learn_buffer is None:\n",
    "            obs_train, value_train = obs, value\n",
    "        else:\n",
    "            obs_train = np.concatenate([obs, self.value_learn_buffer[0]])\n",
    "            value_train = np.concatenate([value, self.value_learn_buffer[1]])\n",
    "        self.value_learn_buffer = (obs, value)\n",
    "\n",
    "        all_loss = []\n",
    "        for _ in range(self.value_learn_times):\n",
    "            random_ids = np.arange(obs_train.shape[0])\n",
    "            np.random.shuffle(random_ids)\n",
    "            shuffle_obs_train = obs_train[random_ids]\n",
    "            shuffle_value_train = value_train[random_ids]\n",
    "            start = 0\n",
    "            while start < data_size:\n",
    "                end = start + self.value_batch_size\n",
    "                value_loss = self._batch_value_learn(\n",
    "                    shuffle_obs_train[start:end, :],\n",
    "                    shuffle_value_train[start:end])\n",
    "                all_loss.append(value_loss)\n",
    "                start += self.value_batch_size\n",
    "        return np.mean(all_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "\n",
    "__all__ = ['Scaler']\n",
    "\n",
    "\n",
    "class Scaler(object):\n",
    "    \"\"\" Generate scale and offset based on running mean and stddev along axis=0\n",
    "        offset = running mean\n",
    "        scale = 1 / (stddev + 0.1) / 3 (i.e. 3x stddev = +/- 1.0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            obs_dim: dimension of axis=1\n",
    "        \"\"\"\n",
    "        self.vars = np.zeros(obs_dim)\n",
    "        self.means = np.zeros(obs_dim)\n",
    "        self.cnt = 0\n",
    "        self.first_pass = True\n",
    "\n",
    "    def update(self, x):\n",
    "        \"\"\" Update running mean and variance (this is an exact method)\n",
    "        Args:\n",
    "            x: NumPy array, shape = (N, obs_dim)\n",
    "        see: https://stats.stackexchange.com/questions/43159/how-to-calculate-pooled-\n",
    "               variance-of-two-groups-given-known-group-variances-mean\n",
    "        \"\"\"\n",
    "        if self.first_pass:\n",
    "            self.means = np.mean(x, axis=0)\n",
    "            self.vars = np.var(x, axis=0)\n",
    "            self.cnt = x.shape[0]\n",
    "            self.first_pass = False\n",
    "        else:\n",
    "            n = x.shape[0]\n",
    "            new_data_var = np.var(x, axis=0)\n",
    "            new_data_mean = np.mean(x, axis=0)\n",
    "            new_data_mean_sq = np.square(new_data_mean)\n",
    "            new_means = (\n",
    "                (self.means * self.cnt) + (new_data_mean * n)) / (self.cnt + n)\n",
    "            self.vars = (((self.cnt * (self.vars + np.square(self.means))) +\n",
    "                          (n * (new_data_var + new_data_mean_sq))) /\n",
    "                         (self.cnt + n) - np.square(new_means))\n",
    "            self.vars = np.maximum(\n",
    "                0.0, self.vars)  # occasionally goes negative, clip\n",
    "            self.means = new_means\n",
    "            self.cnt += n\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\" returns 2-tuple: (scale, offset) \"\"\"\n",
    "        return 1 / (np.sqrt(self.vars) + 0.1) / 3, self.means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "import parl\n",
    "from parl.utils import logger, action_mapping\n",
    "from parl.utils.rl_utils import calc_gae, calc_discount_sum_rewards\n",
    "\n",
    "\n",
    "def run_train_episode(env, agent, scaler):\n",
    "    obs = env.reset()\n",
    "    observes, actions, rewards, unscaled_obs = [], [], [], []\n",
    "    step = 0.0\n",
    "    scale, offset = scaler.get()\n",
    "    scale[-1] = 1.0  # don't scale time step feature\n",
    "    offset[-1] = 0.0  # don't offset time step feature\n",
    "    env.reset()\n",
    "    while True:\n",
    "        obs = obs.reshape((1, -1))\n",
    "        obs = np.append(obs, [[step]], axis=1)  # add time step feature\n",
    "        unscaled_obs.append(obs)\n",
    "        obs = (obs - offset) * scale  # center and scale observations\n",
    "        obs = obs.astype('float32')\n",
    "        observes.append(obs)\n",
    "\n",
    "        action = agent.policy_sample(obs)\n",
    "        action = np.clip(action, -1.0, 1.0)\n",
    "        action = action_mapping(action, env.action_space.low[0],\n",
    "                                env.action_space.high[0])\n",
    "\n",
    "        action = action.reshape((1, -1)).astype('float32')\n",
    "        env.render()\n",
    "        actions.append(action)\n",
    "\n",
    "        obs, reward, done, _ = env.step(np.squeeze(action))\n",
    "        rewards.append(reward)\n",
    "        step += 1e-3  # increment time step feature\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return (np.concatenate(observes), np.concatenate(actions),\n",
    "            np.array(rewards, dtype='float32'), np.concatenate(unscaled_obs))\n",
    "\n",
    "\n",
    "def run_evaluate_episode(env, agent, scaler):\n",
    "    obs = env.reset()\n",
    "    rewards = []\n",
    "    step = 0.0\n",
    "    scale, offset = scaler.get()\n",
    "    scale[-1] = 1.0  # don't scale time step feature\n",
    "    offset[-1] = 0.0  # don't offset time step feature\n",
    "    while True:\n",
    "        obs = obs.reshape((1, -1))\n",
    "        obs = np.append(obs, [[step]], axis=1)  # add time step feature\n",
    "        obs = (obs - offset) * scale  # center and scale observations\n",
    "        obs = obs.astype('float32')\n",
    "\n",
    "        action = agent.policy_predict(obs)\n",
    "        action = action_mapping(action, env.action_space.low[0],\n",
    "                                env.action_space.high[0])\n",
    "\n",
    "        obs, reward, done, _ = env.step(np.squeeze(action))\n",
    "        env.render()\n",
    "        rewards.append(reward)\n",
    "\n",
    "        step += 1e-3  # increment time step feature\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    return np.sum(rewards)\n",
    "\n",
    "\n",
    "def collect_trajectories(env, agent, scaler, episodes):\n",
    "    trajectories, all_unscaled_obs = [], []\n",
    "    for e in range(episodes):\n",
    "        obs, actions, rewards, unscaled_obs = run_train_episode(\n",
    "            env, agent, scaler)\n",
    "        trajectories.append({\n",
    "            'obs': obs,\n",
    "            'actions': actions,\n",
    "            'rewards': rewards,\n",
    "        })\n",
    "        all_unscaled_obs.append(unscaled_obs)\n",
    "    # update running statistics for scaling observations\n",
    "    scaler.update(np.concatenate(all_unscaled_obs))\n",
    "    return trajectories\n",
    "\n",
    "\n",
    "def build_train_data(trajectories, agent):\n",
    "    train_obs, train_actions, train_advantages, train_discount_sum_rewards = [], [], [], []\n",
    "    for trajectory in trajectories:\n",
    "        pred_values = agent.value_predict(trajectory['obs'])\n",
    "\n",
    "        # scale rewards\n",
    "        scale_rewards = trajectory['rewards'] * (1 - 0.995)\n",
    "\n",
    "        discount_sum_rewards = calc_discount_sum_rewards(\n",
    "            scale_rewards, 0.995).astype('float32')\n",
    "\n",
    "        advantages = calc_gae(scale_rewards, pred_values, 0, 0.995,\n",
    "                              0.98)\n",
    "\n",
    "        # normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (\n",
    "            advantages.std() + 1e-6)\n",
    "        advantages = advantages.astype('float32')\n",
    "\n",
    "        train_obs.append(trajectory['obs'])\n",
    "        train_actions.append(trajectory['actions'])\n",
    "        train_advantages.append(advantages)\n",
    "        train_discount_sum_rewards.append(discount_sum_rewards)\n",
    "\n",
    "    train_obs = np.concatenate(train_obs)\n",
    "    train_actions = np.concatenate(train_actions)\n",
    "    train_advantages = np.concatenate(train_advantages)\n",
    "    train_discount_sum_rewards = np.concatenate(train_discount_sum_rewards)\n",
    "\n",
    "    return train_obs, train_actions, train_advantages, train_discount_sum_rewards\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06-28 13:33:16 MainThread @machine_info.py:88]\u001b[0m Cannot find available GPU devices, using CPU now.\n",
      "name: \"obs\"\n",
      "type {\n",
      "  type: LOD_TENSOR\n",
      "  lod_tensor {\n",
      "    tensor {\n",
      "      data_type: FP32\n",
      "      dims: -1\n",
      "      dims: 25\n",
      "    }\n",
      "    lod_level: 0\n",
      "  }\n",
      "}\n",
      "persistable: false\n",
      "\n",
      "name: \"fc_1.tmp_2\"\n",
      "type {\n",
      "  type: LOD_TENSOR\n",
      "  lod_tensor {\n",
      "    tensor {\n",
      "      data_type: FP32\n",
      "      dims: -1\n",
      "      dims: 4\n",
      "    }\n",
      "    lod_level: 0\n",
      "  }\n",
      "}\n",
      "persistable: false\n",
      " name: \"create_parameter.w_0\"\n",
      "type {\n",
      "  type: LOD_TENSOR\n",
      "  lod_tensor {\n",
      "    tensor {\n",
      "      data_type: FP32\n",
      "      dims: 4\n",
      "    }\n",
      "  }\n",
      "}\n",
      "persistable: true\n",
      "\n",
      "aaaaa\n",
      "\u001b[32m[06-28 13:33:16 MainThread @machine_info.py:88]\u001b[0m Cannot find available GPU devices, using CPU now.\n",
      "\u001b[32m[06-28 13:36:09 MainThread @machine_info.py:88]\u001b[0m Cannot find available GPU devices, using CPU now.\n",
      "\u001b[32m[06-28 13:36:11 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1999, Evaluate reward: -96.7204967762089\n",
      "\u001b[32m[06-28 13:39:06 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 10836, Evaluate reward: -33.02540654165919\n",
      "\u001b[32m[06-28 13:42:13 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 20745, Evaluate reward: -31.70261502403704\n",
      "\u001b[32m[06-28 13:45:52 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 32594, Evaluate reward: -33.699070468084265\n",
      "\u001b[32m[06-28 13:49:23 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 44114, Evaluate reward: -40.03193595776273\n",
      "\u001b[32m[06-28 13:51:57 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 52114, Evaluate reward: -40.75602132379885\n",
      "\u001b[32m[06-28 13:54:31 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 60114, Evaluate reward: -28.684756563600153\n",
      "\u001b[32m[06-28 13:58:02 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 73064, Evaluate reward: -106.78592780298057\n",
      "\u001b[32m[06-28 14:01:10 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 84582, Evaluate reward: -117.46135723810829\n",
      "\u001b[32m[06-28 14:03:19 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 91049, Evaluate reward: -40.91126600047885\n",
      "\u001b[32m[06-28 14:06:50 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 102530, Evaluate reward: -48.4334344985988\n",
      "\u001b[32m[06-28 14:09:24 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 110530, Evaluate reward: -46.61110315985419\n",
      "\u001b[32m[06-28 14:13:42 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 125028, Evaluate reward: -44.80173486032585\n",
      "\u001b[32m[06-28 14:15:54 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 133028, Evaluate reward: -125.77844145851583\n",
      "\u001b[32m[06-28 14:18:27 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 141028, Evaluate reward: -51.4374016133553\n",
      "\u001b[32m[06-28 14:23:11 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 157028, Evaluate reward: -48.299852997266996\n",
      "\u001b[32m[06-28 14:25:47 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 165028, Evaluate reward: -60.00388825993924\n",
      "\u001b[32m[06-28 14:28:28 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 173028, Evaluate reward: -53.852748112006736\n",
      "\u001b[32m[06-28 14:31:10 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 181028, Evaluate reward: -53.04794205913879\n",
      "\u001b[32m[06-28 14:35:42 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 195497, Evaluate reward: -48.62690094678601\n",
      "\u001b[32m[06-28 14:38:44 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 203497, Evaluate reward: -50.865793713842834\n",
      "\u001b[32m[06-28 14:41:19 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 211497, Evaluate reward: -52.90546798427465\n",
      "\u001b[32m[06-28 14:46:09 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 227497, Evaluate reward: -47.716435830893616\n",
      "\u001b[32m[06-28 14:48:48 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 235497, Evaluate reward: -47.17000467653014\n",
      "\u001b[32m[06-28 14:51:29 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 241973, Evaluate reward: -48.156215040864424\n",
      "\u001b[32m[06-28 14:55:55 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 257973, Evaluate reward: -50.694333915593724\n",
      "\u001b[32m[06-28 14:58:18 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 265973, Evaluate reward: -50.299936481469004\n",
      "\u001b[32m[06-28 15:00:17 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 272443, Evaluate reward: -50.16524786177216\n",
      "\u001b[32m[06-28 15:02:39 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 280443, Evaluate reward: -47.387474795890355\n",
      "\u001b[32m[06-28 15:07:53 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 296320, Evaluate reward: -43.784251854379356\n",
      "\u001b[32m[06-28 15:10:41 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 303954, Evaluate reward: -45.87089077637593\n",
      "\u001b[32m[06-28 15:13:20 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 311384, Evaluate reward: -45.120045836692675\n",
      "\u001b[32m[06-28 15:18:48 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 327384, Evaluate reward: -45.749805949659404\n",
      "\u001b[32m[06-28 15:22:20 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 333860, Evaluate reward: -40.09239692184702\n",
      "\u001b[32m[06-28 15:25:32 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 341860, Evaluate reward: -40.84337972700162\n",
      "\u001b[32m[06-28 15:36:11 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 357860, Evaluate reward: -40.828537029603496\n",
      "\u001b[32m[06-28 15:39:06 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 364431, Evaluate reward: -33.69393901508177\n",
      "\u001b[32m[06-28 15:42:17 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 372431, Evaluate reward: -34.66207428511419\n",
      "\u001b[32m[06-28 15:44:57 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 380431, Evaluate reward: 25.07357213407134\n",
      "\u001b[32m[06-28 15:50:54 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 396431, Evaluate reward: -123.00512594141502\n",
      "\u001b[32m[06-28 15:54:31 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 404431, Evaluate reward: 149.90212663618289\n",
      "\u001b[32m[06-28 15:57:47 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 412431, Evaluate reward: 158.636982754274\n",
      "\u001b[32m[06-28 16:01:12 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 420431, Evaluate reward: 182.83055566189378\n",
      "\u001b[32m[06-28 16:06:14 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 435876, Evaluate reward: 68.65448695790704\n",
      "\u001b[32m[06-28 16:09:07 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 443876, Evaluate reward: 133.36417254115395\n",
      "\u001b[32m[06-28 16:11:25 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 450033, Evaluate reward: 202.67893242856795\n",
      "\u001b[32m[06-28 16:16:14 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 464541, Evaluate reward: 207.4722861449805\n",
      "\u001b[32m[06-28 16:20:10 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 472541, Evaluate reward: 206.07193494388338\n",
      "\u001b[32m[06-28 16:26:41 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 480541, Evaluate reward: 212.50752919280217\n",
      "\u001b[32m[06-28 16:34:59 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 496541, Evaluate reward: 202.81305790839906\n",
      "\u001b[32m[06-28 16:39:15 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 504541, Evaluate reward: 195.397432560175\n",
      "\u001b[32m[06-28 16:43:59 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 512541, Evaluate reward: 218.20729629789975\n",
      "\u001b[32m[06-28 16:48:15 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 520541, Evaluate reward: 207.55166490355504\n",
      "\u001b[32m[06-28 16:54:30 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 532656, Evaluate reward: 221.2276193806411\n",
      "\u001b[32m[06-28 17:01:40 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 547134, Evaluate reward: 90.03245788069567\n",
      "\u001b[32m[06-28 17:06:46 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 554782, Evaluate reward: 224.7203193820988\n",
      "\u001b[32m[06-28 17:12:33 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 562514, Evaluate reward: 215.27892492389552\n",
      "\u001b[32m[06-28 17:17:07 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 570514, Evaluate reward: 215.5147895740531\n",
      "\u001b[32m[06-28 17:25:43 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 586514, Evaluate reward: 220.47019817862167\n",
      "\u001b[32m[06-28 17:30:28 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 594514, Evaluate reward: 214.95033341331217\n",
      "\u001b[32m[06-28 17:35:08 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 600963, Evaluate reward: 224.52380363153355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06-28 17:44:44 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 615422, Evaluate reward: 223.49776001399385\n",
      "\u001b[32m[06-28 17:48:31 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 621904, Evaluate reward: 218.64807705426216\n",
      "\u001b[32m[06-28 17:56:06 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 637904, Evaluate reward: 206.47997165803673\n",
      "\u001b[32m[06-28 18:00:21 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 645904, Evaluate reward: 225.8231842094157\n",
      "\u001b[32m[06-28 18:04:24 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 653752, Evaluate reward: 207.51571455761604\n",
      "\u001b[32m[06-28 18:08:55 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 661752, Evaluate reward: 223.08589108535955\n",
      "\u001b[32m[06-28 18:15:51 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 676236, Evaluate reward: 225.00745830753942\n",
      "\u001b[32m[06-28 18:19:56 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 684236, Evaluate reward: 236.4745701614848\n",
      "\u001b[32m[06-28 18:24:17 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 692236, Evaluate reward: 227.14593782326764\n",
      "\u001b[32m[06-28 18:28:50 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 700236, Evaluate reward: 220.88515783897412\n",
      "\u001b[32m[06-28 18:36:44 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 716236, Evaluate reward: 211.01169758573425\n",
      "\u001b[32m[06-28 18:41:18 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 724236, Evaluate reward: 215.11598651318624\n",
      "\u001b[32m[06-28 18:46:27 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 732236, Evaluate reward: 223.5110544320525\n",
      "\u001b[32m[06-28 18:54:18 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 747526, Evaluate reward: 218.58923493779082\n",
      "\u001b[32m[06-28 18:59:33 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 755526, Evaluate reward: 205.1705276406513\n",
      "\u001b[32m[06-28 19:03:50 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 763526, Evaluate reward: 203.77419124162262\n",
      "\u001b[32m[06-28 19:08:20 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 771526, Evaluate reward: 216.62730829718274\n",
      "\u001b[32m[06-28 19:15:39 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 785995, Evaluate reward: 217.6027039719596\n",
      "\u001b[32m[06-28 19:19:26 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 793710, Evaluate reward: 220.16787827471333\n",
      "\u001b[32m[06-28 19:23:28 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 801710, Evaluate reward: 214.12130918773016\n",
      "\u001b[32m[06-28 19:30:34 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 815628, Evaluate reward: 214.46135289846487\n",
      "\u001b[32m[06-28 19:34:40 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 823628, Evaluate reward: 210.78118983146737\n",
      "\u001b[32m[06-28 19:38:47 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 831628, Evaluate reward: 209.2600868268485\n",
      "\u001b[32m[06-28 19:46:49 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 847628, Evaluate reward: 219.2488242752819\n",
      "\u001b[32m[06-28 19:50:50 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 855628, Evaluate reward: 223.76432687030425\n",
      "\u001b[32m[06-28 19:54:42 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 863628, Evaluate reward: 222.22597839604566\n",
      "\u001b[32m[06-28 19:58:19 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 870186, Evaluate reward: 219.0451962015455\n",
      "\u001b[32m[06-28 20:06:02 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 886186, Evaluate reward: 231.24567348635506\n",
      "\u001b[32m[06-28 20:10:23 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 893573, Evaluate reward: 227.63699730027838\n",
      "\u001b[32m[06-28 20:14:09 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 900966, Evaluate reward: 212.0660012277681\n",
      "\u001b[32m[06-28 20:21:53 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 916966, Evaluate reward: 195.41016969594108\n",
      "\u001b[32m[06-28 20:25:42 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 924591, Evaluate reward: 203.70885312647437\n",
      "\u001b[32m[06-28 20:29:03 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 931439, Evaluate reward: 213.75383305152567\n",
      "\u001b[32m[06-28 20:36:32 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 947439, Evaluate reward: 204.72192353810868\n",
      "\u001b[32m[06-28 20:40:59 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 955439, Evaluate reward: 209.89759836210496\n",
      "\u001b[32m[06-28 20:48:05 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 963439, Evaluate reward: 210.9552082118696\n",
      "\u001b[32m[06-28 20:55:54 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 970999, Evaluate reward: 221.3111595776876\n",
      "\u001b[32m[06-28 21:03:32 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 986999, Evaluate reward: 221.89280749973278\n",
      "\u001b[32m[06-28 21:07:36 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 993458, Evaluate reward: 228.19903792399538\n",
      "\u001b[32m[06-28 21:12:00 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1001458, Evaluate reward: 225.1027258469984\n",
      "\u001b[32m[06-28 21:19:54 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1015951, Evaluate reward: 222.43984797396885\n",
      "\u001b[32m[06-28 21:25:22 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1023951, Evaluate reward: 198.4894441560184\n",
      "\u001b[32m[06-28 21:30:55 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1031951, Evaluate reward: 215.66068157347786\n",
      "\u001b[32m[06-28 21:38:57 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1046869, Evaluate reward: 206.8613631325321\n",
      "\u001b[32m[06-28 21:42:43 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1054763, Evaluate reward: 197.76021815207974\n",
      "\u001b[32m[06-28 21:46:39 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1062763, Evaluate reward: 209.9169020743047\n",
      "\u001b[32m[06-28 21:50:30 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1070763, Evaluate reward: 212.59007154206557\n",
      "\u001b[32m[06-28 21:57:17 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1086051, Evaluate reward: 221.10398777814896\n",
      "\u001b[32m[06-28 22:00:59 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1093696, Evaluate reward: 212.19944927799702\n",
      "\u001b[32m[06-28 22:04:50 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1101696, Evaluate reward: 219.0324527270719\n",
      "\u001b[32m[06-28 22:11:40 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1117140, Evaluate reward: 233.31529598228946\n",
      "\u001b[32m[06-28 22:15:31 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1125140, Evaluate reward: 252.71877148808045\n",
      "\u001b[32m[06-28 22:18:07 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1130053, Evaluate reward: 227.59574074641364\n",
      "\u001b[32m[06-28 22:25:03 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1146053, Evaluate reward: 211.70913296155072\n",
      "\u001b[32m[06-28 22:28:51 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1154053, Evaluate reward: 217.55996611419317\n",
      "\u001b[32m[06-28 22:32:39 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1162053, Evaluate reward: 226.1556314863109\n",
      "\u001b[32m[06-28 22:36:29 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1170053, Evaluate reward: 213.50718266299117\n",
      "\u001b[32m[06-28 22:43:28 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1186053, Evaluate reward: 222.20964784110896\n",
      "\u001b[32m[06-28 22:47:15 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1194053, Evaluate reward: 211.03325495842597\n",
      "\u001b[32m[06-28 22:51:02 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1202053, Evaluate reward: 221.85389438918358\n",
      "\u001b[32m[06-28 22:57:27 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1216619, Evaluate reward: 229.1074427622625\n",
      "\u001b[32m[06-28 23:01:14 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1224619, Evaluate reward: 221.45969839417995\n",
      "\u001b[32m[06-28 23:05:02 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1232619, Evaluate reward: 224.86555382080812\n",
      "\u001b[32m[06-28 23:08:53 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1240619, Evaluate reward: 241.24447952279445\n",
      "\u001b[32m[06-28 23:15:51 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1256619, Evaluate reward: 244.2122965582684\n",
      "\u001b[32m[06-28 23:19:04 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1263198, Evaluate reward: 250.39373984717514\n",
      "\u001b[32m[06-28 23:22:32 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1270306, Evaluate reward: 251.54630608960616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06-28 23:29:28 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1286306, Evaluate reward: 246.978987059148\n",
      "\u001b[32m[06-28 23:33:17 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1294306, Evaluate reward: 251.42812024933164\n",
      "\u001b[32m[06-28 23:36:58 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1302306, Evaluate reward: 230.75237336429393\n",
      "\u001b[32m[06-28 23:40:44 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1310306, Evaluate reward: 242.91636031022665\n",
      "\u001b[32m[06-28 23:47:51 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1326306, Evaluate reward: 219.8094091451876\n",
      "\u001b[32m[06-28 23:51:47 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1334306, Evaluate reward: 234.6478143842537\n",
      "\u001b[32m[06-28 23:55:33 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1342306, Evaluate reward: 244.91088921142554\n",
      "\u001b[32m[06-28 23:59:20 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1350271, Evaluate reward: 238.58095465933346\n",
      "\u001b[32m[06-29 00:06:08 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1365964, Evaluate reward: 236.00931719314556\n",
      "\u001b[32m[06-29 00:09:56 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1373964, Evaluate reward: 225.75420397183362\n",
      "\u001b[32m[06-29 00:13:43 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1381964, Evaluate reward: 228.13204123970002\n",
      "\u001b[32m[06-29 00:20:36 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1397964, Evaluate reward: 220.3841065916314\n",
      "\u001b[32m[06-29 00:24:23 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1405964, Evaluate reward: 226.05788587417828\n",
      "\u001b[32m[06-29 00:28:05 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1413964, Evaluate reward: 242.18630521983408\n",
      "\u001b[32m[06-29 00:31:51 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1421964, Evaluate reward: 258.47836419867167\n",
      "\u001b[32m[06-29 00:38:43 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1437964, Evaluate reward: 253.9784466996367\n",
      "\u001b[32m[06-29 00:42:30 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1445964, Evaluate reward: 253.21270501902882\n",
      "\u001b[32m[06-29 00:46:17 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1453964, Evaluate reward: 239.7880495942837\n",
      "\u001b[32m[06-29 00:49:58 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1461964, Evaluate reward: 240.99475521343385\n",
      "\u001b[32m[06-29 00:56:51 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1477869, Evaluate reward: 268.30469432310576\n",
      "\u001b[32m[06-29 01:00:32 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1485869, Evaluate reward: 247.25796928180566\n",
      "\u001b[32m[06-29 01:04:17 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1493869, Evaluate reward: 254.62709702916072\n",
      "\u001b[32m[06-29 01:08:01 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1501869, Evaluate reward: 265.8980069220215\n",
      "\u001b[32m[06-29 01:14:50 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1517869, Evaluate reward: 247.65452526079676\n",
      "\u001b[32m[06-29 01:16:57 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1522782, Evaluate reward: -85.38101121701735\n",
      "\u001b[32m[06-29 01:20:39 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1530782, Evaluate reward: 214.43493417939294\n",
      "\u001b[32m[06-29 01:25:27 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1541440, Evaluate reward: 272.79222849907603\n",
      "\u001b[32m[06-29 01:32:16 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1557440, Evaluate reward: 271.9030019182166\n",
      "\u001b[32m[06-29 01:35:59 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1565440, Evaluate reward: 272.5974069612815\n",
      "\u001b[32m[06-29 01:39:36 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1573440, Evaluate reward: 271.6111319111878\n",
      "\u001b[32m[06-29 01:42:56 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1581440, Evaluate reward: -6.528762764630223\n",
      "\u001b[32m[06-29 01:48:54 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1595335, Evaluate reward: 270.9314831758415\n",
      "\u001b[32m[06-29 01:52:39 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1603335, Evaluate reward: 268.6196270604655\n",
      "\u001b[32m[06-29 01:56:05 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1611335, Evaluate reward: 21.567760706840076\n",
      "\u001b[32m[06-29 02:02:24 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1626713, Evaluate reward: 12.478917119648294\n",
      "\u001b[32m[06-29 02:05:54 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1634062, Evaluate reward: 213.18050015768284\n",
      "\u001b[32m[06-29 02:09:36 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1641977, Evaluate reward: 266.5791889404338\n",
      "\u001b[32m[06-29 02:16:16 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1657977, Evaluate reward: 44.033676399343975\n",
      "\u001b[32m[06-29 02:19:33 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1664743, Evaluate reward: 255.63458783937622\n",
      "\u001b[32m[06-29 02:22:50 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1672553, Evaluate reward: -23.375119149902822\n",
      "\u001b[32m[06-29 02:26:30 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1680482, Evaluate reward: 274.8233229394226\n",
      "\u001b[32m[06-29 02:33:12 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1696049, Evaluate reward: 136.74518623624493\n",
      "\u001b[32m[06-29 02:36:37 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1703222, Evaluate reward: 262.03302403324284\n",
      "\u001b[32m[06-29 02:40:18 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1711222, Evaluate reward: 253.6641423841367\n",
      "\u001b[32m[06-29 02:47:10 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1727222, Evaluate reward: 267.5016965955483\n",
      "\u001b[32m[06-29 02:50:54 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1735222, Evaluate reward: 258.9584426111511\n",
      "\u001b[32m[06-29 02:54:25 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1743222, Evaluate reward: 56.588519608244944\n",
      "\u001b[32m[06-29 02:58:13 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1751222, Evaluate reward: 227.65737624466357\n",
      "\u001b[32m[06-29 03:05:02 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1767222, Evaluate reward: 275.0852256635807\n",
      "\u001b[32m[06-29 03:08:44 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1775222, Evaluate reward: 280.4324778314189\n",
      "\u001b[32m[06-29 03:12:00 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1783222, Evaluate reward: -41.07804226482287\n",
      "\u001b[32m[06-29 03:17:41 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1796601, Evaluate reward: 78.84455186397767\n",
      "\u001b[32m[06-29 03:21:04 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1804601, Evaluate reward: 19.11099572041507\n",
      "\u001b[32m[06-29 03:24:38 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1812601, Evaluate reward: 70.16164513586274\n",
      "\u001b[32m[06-29 03:28:02 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1820601, Evaluate reward: -68.45309399459885\n",
      "\u001b[32m[06-29 03:34:36 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1836374, Evaluate reward: -25.394743819092085\n",
      "\u001b[32m[06-29 03:37:51 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1844231, Evaluate reward: -62.95919441030795\n",
      "\u001b[32m[06-29 03:41:06 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1852231, Evaluate reward: -80.50192522618981\n",
      "\u001b[32m[06-29 03:44:48 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1860231, Evaluate reward: 238.1828294864489\n",
      "\u001b[32m[06-29 03:51:43 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1876231, Evaluate reward: 207.94886860717713\n",
      "\u001b[32m[06-29 03:55:00 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1882960, Evaluate reward: 236.43263715108358\n",
      "\u001b[32m[06-29 03:58:45 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1890960, Evaluate reward: 234.0604565090419\n",
      "\u001b[32m[06-29 04:05:29 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1906542, Evaluate reward: 223.41274855402548\n",
      "\u001b[32m[06-29 04:09:13 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1914542, Evaluate reward: 266.2728108388676\n",
      "\u001b[32m[06-29 04:12:59 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1922542, Evaluate reward: 224.37412523749222\n",
      "\u001b[32m[06-29 04:16:45 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1930542, Evaluate reward: 236.89806317278368\n",
      "\u001b[32m[06-29 04:22:29 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1943483, Evaluate reward: 231.46467209525218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06-29 04:25:45 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1950241, Evaluate reward: 219.7972831671654\n",
      "\u001b[32m[06-29 04:32:40 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1966241, Evaluate reward: 234.97775551762248\n",
      "\u001b[32m[06-29 04:36:26 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1974241, Evaluate reward: 215.33524045896155\n",
      "\u001b[32m[06-29 04:39:57 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1981739, Evaluate reward: 112.19274581854543\n",
      "\u001b[32m[06-29 04:46:48 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 1997595, Evaluate reward: 267.26331055185074\n",
      "\u001b[32m[06-29 04:50:31 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2005539, Evaluate reward: 268.63409972513904\n",
      "\u001b[32m[06-29 04:54:06 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2013155, Evaluate reward: 263.0783014684369\n",
      "\u001b[32m[06-29 04:57:51 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2021155, Evaluate reward: 260.1788816907574\n",
      "\u001b[32m[06-29 05:04:46 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2037155, Evaluate reward: 257.591867548426\n",
      "\u001b[32m[06-29 05:08:33 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2045155, Evaluate reward: 250.56904428484603\n",
      "\u001b[32m[06-29 05:12:18 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2053155, Evaluate reward: 250.16157347312878\n",
      "\u001b[32m[06-29 05:16:04 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2061155, Evaluate reward: 253.24291583801434\n",
      "\u001b[32m[06-29 05:23:02 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2077155, Evaluate reward: 258.03013191169873\n",
      "\u001b[32m[06-29 05:26:19 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2084017, Evaluate reward: 258.0860225881996\n",
      "\u001b[32m[06-29 05:30:06 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2092017, Evaluate reward: 222.18306290613052\n",
      "\u001b[32m[06-29 05:35:59 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2105413, Evaluate reward: 249.8796695036596\n",
      "\u001b[32m[06-29 05:39:43 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2113413, Evaluate reward: 117.98087489370323\n",
      "\u001b[32m[06-29 05:43:30 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2121413, Evaluate reward: 237.5476329238986\n",
      "\u001b[32m[06-29 05:49:24 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2135016, Evaluate reward: 62.996603076523186\n",
      "\u001b[32m[06-29 05:53:11 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2143016, Evaluate reward: 255.2165958776704\n",
      "\u001b[32m[06-29 05:56:22 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2150158, Evaluate reward: 24.125656615155442\n",
      "\u001b[32m[06-29 06:02:45 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2164661, Evaluate reward: 217.84231149302116\n",
      "\u001b[32m[06-29 06:06:31 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2172588, Evaluate reward: 247.60162009932597\n",
      "\u001b[32m[06-29 06:10:12 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2180401, Evaluate reward: 255.0124319944785\n",
      "\u001b[32m[06-29 06:17:00 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2196026, Evaluate reward: 137.15649176457526\n",
      "\u001b[32m[06-29 06:20:46 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2204026, Evaluate reward: 244.11693480854913\n",
      "\u001b[32m[06-29 06:23:59 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2212026, Evaluate reward: -87.50442523467602\n",
      "\u001b[32m[06-29 06:30:13 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2226183, Evaluate reward: 251.89914601724905\n",
      "\u001b[32m[06-29 06:33:45 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2234183, Evaluate reward: 18.96142573624973\n",
      "\u001b[32m[06-29 06:37:28 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2242183, Evaluate reward: 261.8059879047845\n",
      "\u001b[32m[06-29 06:41:13 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2250183, Evaluate reward: 254.72048613327678\n",
      "\u001b[32m[06-29 06:47:01 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2263801, Evaluate reward: 109.50670278084965\n",
      "\u001b[32m[06-29 06:49:51 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2270465, Evaluate reward: -25.93025445457362\n",
      "\u001b[32m[06-29 06:55:11 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2283613, Evaluate reward: -53.27862940137274\n",
      "\u001b[32m[06-29 07:00:52 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2296490, Evaluate reward: 126.40579322892427\n",
      "\u001b[32m[06-29 07:04:04 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2303151, Evaluate reward: 257.3409257903155\n",
      "\u001b[32m[06-29 07:07:36 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2310904, Evaluate reward: 90.13640746511521\n",
      "\u001b[32m[06-29 07:13:59 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2325580, Evaluate reward: 232.75998374718802\n",
      "\u001b[32m[06-29 07:17:27 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2333045, Evaluate reward: 210.9550718783842\n",
      "\u001b[32m[06-29 07:23:51 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2347691, Evaluate reward: 253.68286093457482\n",
      "\u001b[32m[06-29 07:27:20 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2355691, Evaluate reward: 29.209481557120277\n",
      "\u001b[32m[06-29 07:30:16 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2361415, Evaluate reward: 255.06126444120395\n",
      "\u001b[32m[06-29 07:36:50 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2376477, Evaluate reward: 257.8498311605714\n",
      "\u001b[32m[06-29 07:40:29 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2384477, Evaluate reward: 250.46251019188264\n",
      "\u001b[32m[06-29 07:43:34 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2390704, Evaluate reward: 245.60934265092325\n",
      "\u001b[32m[06-29 07:50:23 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2406704, Evaluate reward: 252.50444905949817\n",
      "\u001b[32m[06-29 07:54:11 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2414704, Evaluate reward: 249.09338037670034\n",
      "\u001b[32m[06-29 07:57:57 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2422704, Evaluate reward: 252.6906041105073\n",
      "\u001b[32m[06-29 08:01:33 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2430481, Evaluate reward: 253.98444752583092\n",
      "\u001b[32m[06-29 08:07:31 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2443732, Evaluate reward: 244.69756954430667\n",
      "\u001b[32m[06-29 08:10:46 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2450577, Evaluate reward: 239.44747566495215\n",
      "\u001b[32m[06-29 08:16:56 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2464303, Evaluate reward: 244.73509167975436\n",
      "\u001b[32m[06-29 08:20:42 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2472303, Evaluate reward: 251.6312565635648\n",
      "\u001b[32m[06-29 08:27:28 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2487777, Evaluate reward: 257.9526542583623\n",
      "\u001b[32m[06-29 08:31:12 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2495777, Evaluate reward: 254.77146376631097\n",
      "\u001b[32m[06-29 08:34:39 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2502952, Evaluate reward: 256.7348433777752\n",
      "\u001b[32m[06-29 08:38:24 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2510952, Evaluate reward: 256.90054478317063\n",
      "\u001b[32m[06-29 08:45:21 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2526952, Evaluate reward: 254.75891148890366\n",
      "\u001b[32m[06-29 08:49:10 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2534952, Evaluate reward: 252.86869265728257\n",
      "\u001b[32m[06-29 08:52:10 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2541226, Evaluate reward: 260.4957722067876\n",
      "\u001b[32m[06-29 08:58:20 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2555052, Evaluate reward: 250.87217539115437\n",
      "\u001b[32m[06-29 09:01:19 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2561129, Evaluate reward: 249.37428534960873\n",
      "\u001b[32m[06-29 09:07:02 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2573923, Evaluate reward: 244.43532239965472\n",
      "\u001b[32m[06-29 09:10:50 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2581914, Evaluate reward: 247.10717516262457\n",
      "\u001b[32m[06-29 09:17:52 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2597914, Evaluate reward: 246.10390023975387\n",
      "\u001b[32m[06-29 09:21:34 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2605758, Evaluate reward: 248.01504407639246\n",
      "\u001b[32m[06-29 09:25:08 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2613274, Evaluate reward: 251.14556642936918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06-29 09:28:57 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2621274, Evaluate reward: 245.3858790047827\n",
      "\u001b[32m[06-29 09:35:25 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2635883, Evaluate reward: 247.54709935103853\n",
      "\u001b[32m[06-29 09:39:45 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2643843, Evaluate reward: 246.144580896991\n",
      "\u001b[32m[06-29 09:44:28 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2651843, Evaluate reward: 243.93687082114505\n",
      "\u001b[32m[06-29 09:52:57 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2667576, Evaluate reward: 240.33544183875247\n",
      "\u001b[32m[06-29 09:57:38 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2675576, Evaluate reward: 241.72994440850678\n",
      "\u001b[32m[06-29 10:02:23 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2683576, Evaluate reward: 242.31045636204323\n",
      "\u001b[32m[06-29 10:06:37 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2691075, Evaluate reward: 244.28606957996757\n",
      "\u001b[32m[06-29 10:13:19 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2707075, Evaluate reward: 241.9151008837285\n",
      "\u001b[32m[06-29 10:16:07 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2714352, Evaluate reward: 246.67834587340178\n",
      "\u001b[32m[06-29 10:19:43 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2722352, Evaluate reward: 246.98863275018647\n",
      "\u001b[32m[06-29 10:25:45 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2736948, Evaluate reward: 39.98845224815916\n",
      "\u001b[32m[06-29 10:29:25 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2744948, Evaluate reward: -52.542608306610944\n",
      "\u001b[32m[06-29 10:32:45 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2752948, Evaluate reward: 245.39295752923934\n",
      "\u001b[32m[06-29 10:38:34 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2767744, Evaluate reward: 246.2492829657467\n",
      "\u001b[32m[06-29 10:42:07 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2775744, Evaluate reward: 247.1444970794438\n",
      "\u001b[32m[06-29 10:45:11 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2783021, Evaluate reward: 246.37524654324167\n",
      "\u001b[32m[06-29 10:48:09 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2791021, Evaluate reward: 245.8512870537794\n",
      "\u001b[32m[06-29 10:53:42 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2806910, Evaluate reward: 246.9633092297061\n",
      "\u001b[32m[06-29 10:56:38 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2814910, Evaluate reward: 250.28714279923895\n",
      "\u001b[32m[06-29 11:01:27 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2822152, Evaluate reward: 250.14011431482498\n",
      "\u001b[32m[06-29 11:08:06 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2837947, Evaluate reward: 250.7803592279007\n",
      "\u001b[32m[06-29 11:12:13 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2845947, Evaluate reward: 248.34796537059918\n",
      "\u001b[32m[06-29 11:14:54 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2852998, Evaluate reward: 250.07172396254657\n",
      "\u001b[32m[06-29 11:18:04 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2860433, Evaluate reward: 244.36789179236814\n",
      "\u001b[32m[06-29 11:23:32 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2875698, Evaluate reward: 249.1688656732788\n",
      "\u001b[32m[06-29 11:26:52 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2883698, Evaluate reward: 250.71797869945692\n",
      "\u001b[32m[06-29 11:29:38 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2891698, Evaluate reward: 259.8142654921698\n",
      "\u001b[32m[06-29 11:34:46 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2906323, Evaluate reward: 256.3989486606506\n",
      "\u001b[32m[06-29 11:38:11 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2914318, Evaluate reward: 147.6480487098824\n",
      "\u001b[32m[06-29 11:42:20 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2921201, Evaluate reward: 121.28405901636863\n",
      "\u001b[32m[06-29 11:52:21 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2937201, Evaluate reward: 247.23417987939592\n",
      "\u001b[32m[06-29 11:56:31 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2945201, Evaluate reward: 255.52838830889448\n",
      "\u001b[32m[06-29 12:00:36 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2953201, Evaluate reward: 260.57875247202503\n",
      "\u001b[32m[06-29 12:04:38 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2961201, Evaluate reward: 252.4213143225343\n",
      "\u001b[32m[06-29 12:12:08 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2976204, Evaluate reward: 253.9508299285757\n",
      "\u001b[32m[06-29 12:15:53 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2982970, Evaluate reward: 260.33580974771763\n",
      "\u001b[32m[06-29 12:21:15 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 2990970, Evaluate reward: 260.6368147119569\n",
      "\u001b[32m[06-29 12:28:49 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3005027, Evaluate reward: 259.64471889713224\n",
      "\u001b[32m[06-29 12:32:00 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3011129, Evaluate reward: 260.5054149898148\n",
      "\u001b[32m[06-29 12:39:29 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3025603, Evaluate reward: 247.4378328859862\n",
      "\u001b[32m[06-29 12:43:42 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3033603, Evaluate reward: 247.14454368258515\n",
      "\u001b[32m[06-29 12:47:47 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3041367, Evaluate reward: 251.5538098263045\n",
      "\u001b[32m[06-29 12:54:29 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3055230, Evaluate reward: 262.08306316863064\n",
      "\u001b[32m[06-29 12:58:12 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3062291, Evaluate reward: 257.29207065788535\n",
      "\u001b[32m[06-29 13:02:26 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3070287, Evaluate reward: 253.77928166613913\n",
      "\u001b[32m[06-29 13:10:04 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3085909, Evaluate reward: 263.4451475608876\n",
      "\u001b[32m[06-29 13:14:24 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3093909, Evaluate reward: 267.03221204015614\n",
      "\u001b[32m[06-29 13:18:14 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3100811, Evaluate reward: 264.77184094616143\n",
      "\u001b[32m[06-29 13:25:23 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3115619, Evaluate reward: 26.935006933816396\n",
      "\u001b[32m[06-29 13:29:47 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3121658, Evaluate reward: 267.5641937850652\n",
      "\u001b[32m[06-29 13:40:57 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3136780, Evaluate reward: 249.11722018735233\n",
      "\u001b[32m[06-29 13:47:17 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3144780, Evaluate reward: 259.1987590307214\n",
      "\u001b[32m[06-29 13:52:59 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3152780, Evaluate reward: 268.0636547316127\n",
      "\u001b[32m[06-29 13:56:47 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3160006, Evaluate reward: 256.52312381682486\n",
      "\u001b[32m[06-29 14:04:09 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3176006, Evaluate reward: 258.4303550092025\n",
      "\u001b[32m[06-29 14:07:27 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3182421, Evaluate reward: 259.2110135881559\n",
      "\u001b[32m[06-29 14:14:07 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3195919, Evaluate reward: 266.1718455739617\n",
      "\u001b[32m[06-29 14:17:58 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3203329, Evaluate reward: 255.7536194154018\n",
      "\u001b[32m[06-29 14:21:31 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3210373, Evaluate reward: 252.25830390348904\n",
      "\u001b[32m[06-29 14:29:10 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3225393, Evaluate reward: 255.3261814103269\n",
      "\u001b[32m[06-29 14:36:03 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3233393, Evaluate reward: 248.63912974586276\n",
      "\u001b[32m[06-29 14:41:49 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3241393, Evaluate reward: 250.52169082168183\n",
      "\u001b[32m[06-29 14:53:02 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3257393, Evaluate reward: 253.3453244365696\n",
      "\u001b[32m[06-29 14:58:14 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3264099, Evaluate reward: 248.28644313442274\n",
      "\u001b[32m[06-29 15:03:51 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3270756, Evaluate reward: 258.697362906271\n",
      "\u001b[32m[06-29 15:17:06 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3286624, Evaluate reward: 264.71601538082956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06-29 15:22:44 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3294612, Evaluate reward: 268.7454922301596\n",
      "\u001b[32m[06-29 15:27:19 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3302612, Evaluate reward: 259.2924377625777\n",
      "\u001b[32m[06-29 15:33:18 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3310612, Evaluate reward: 268.1642251443261\n",
      "\u001b[32m[06-29 15:41:36 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3323321, Evaluate reward: 257.59219513471237\n",
      "\u001b[32m[06-29 15:45:49 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3330137, Evaluate reward: 39.98526110406592\n",
      "\u001b[32m[06-29 15:54:39 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3343910, Evaluate reward: 144.14881027385653\n",
      "\u001b[32m[06-29 15:59:28 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3351043, Evaluate reward: 254.60236954655932\n",
      "\u001b[32m[06-29 16:10:52 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3364967, Evaluate reward: 262.8042495616227\n",
      "\u001b[32m[06-29 16:20:14 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3372731, Evaluate reward: 259.0272474730772\n",
      "\u001b[32m[06-29 16:29:32 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3380671, Evaluate reward: 256.79198168792266\n",
      "\u001b[32m[06-29 16:43:56 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3394685, Evaluate reward: 37.132255591475115\n",
      "\u001b[32m[06-29 16:50:28 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3401697, Evaluate reward: 267.8783127666054\n",
      "\u001b[32m[06-29 17:03:36 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3414053, Evaluate reward: 263.9770996699333\n",
      "\u001b[32m[06-29 17:12:20 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3421922, Evaluate reward: 256.453164250801\n",
      "\u001b[32m[06-29 17:29:55 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3437681, Evaluate reward: 258.98951454538667\n",
      "\u001b[32m[06-29 17:36:51 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3445149, Evaluate reward: 264.239826933189\n",
      "\u001b[32m[06-29 17:43:20 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3452795, Evaluate reward: 262.7254212997407\n",
      "\u001b[32m[06-29 17:56:29 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3466735, Evaluate reward: 263.3809099878054\n",
      "\u001b[32m[06-29 18:03:26 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3474563, Evaluate reward: 256.31513445654014\n",
      "\u001b[32m[06-29 18:09:19 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3481164, Evaluate reward: 253.85444007236197\n",
      "\u001b[32m[06-29 18:24:23 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3496392, Evaluate reward: 261.5980378430635\n",
      "\u001b[32m[06-29 18:35:16 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3503231, Evaluate reward: 259.89845373293883\n",
      "\u001b[32m[06-29 18:41:15 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3511117, Evaluate reward: 257.09362187111685\n",
      "\u001b[32m[06-29 18:50:05 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3526948, Evaluate reward: 260.511292283435\n",
      "\u001b[32m[06-29 18:53:50 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3533227, Evaluate reward: 258.438005794622\n",
      "\u001b[32m[06-29 18:58:03 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3540364, Evaluate reward: 270.3802530788649\n",
      "\u001b[32m[06-29 19:06:02 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3554196, Evaluate reward: 273.909791264719\n",
      "\u001b[32m[06-29 19:12:42 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3567311, Evaluate reward: 260.4878905191912\n",
      "\u001b[32m[06-29 19:16:08 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3573844, Evaluate reward: 77.7374893561285\n",
      "\u001b[32m[06-29 19:21:08 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3580958, Evaluate reward: 274.1835967946146\n",
      "\u001b[32m[06-29 19:26:51 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3592392, Evaluate reward: 269.5440732208422\n",
      "\u001b[32m[06-29 19:30:35 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3600190, Evaluate reward: 264.3960079503562\n",
      "\u001b[32m[06-29 19:37:27 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3615432, Evaluate reward: 261.5106031321219\n",
      "\u001b[32m[06-29 19:41:08 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3623012, Evaluate reward: 271.14766690470344\n",
      "\u001b[32m[06-29 19:44:55 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3630319, Evaluate reward: 265.23233500834743\n",
      "\u001b[32m[06-29 19:53:20 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3645851, Evaluate reward: 271.88796741139083\n",
      "\u001b[32m[06-29 19:58:00 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3652820, Evaluate reward: 98.8195830229111\n",
      "\u001b[32m[06-29 20:06:13 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3667384, Evaluate reward: 264.2502669977881\n",
      "\u001b[32m[06-29 20:12:44 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3673602, Evaluate reward: 257.72534327585436\n",
      "\u001b[32m[06-29 20:18:35 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3681543, Evaluate reward: 255.71420306615653\n",
      "\u001b[32m[06-29 20:27:31 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3695004, Evaluate reward: 263.38177467486815\n",
      "\u001b[32m[06-29 20:32:17 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3702599, Evaluate reward: 266.93973506433144\n",
      "\u001b[32m[06-29 20:37:20 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3710374, Evaluate reward: 262.0608585748828\n",
      "\u001b[32m[06-29 20:45:54 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3724725, Evaluate reward: 262.80847027451796\n",
      "\u001b[32m[06-29 20:50:59 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3732495, Evaluate reward: 263.0602370438488\n",
      "\u001b[32m[06-29 20:56:30 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3740145, Evaluate reward: 256.3196144742071\n",
      "\u001b[32m[06-29 21:05:25 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3754783, Evaluate reward: 258.8742464335269\n",
      "\u001b[32m[06-29 21:10:35 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3762334, Evaluate reward: 260.94432066901277\n",
      "\u001b[32m[06-29 21:16:48 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3770134, Evaluate reward: 262.5810219950297\n",
      "\u001b[32m[06-29 21:26:26 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3785772, Evaluate reward: 263.4581127797601\n",
      "\u001b[32m[06-29 21:31:08 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3793573, Evaluate reward: 261.21061376179443\n",
      "\u001b[32m[06-29 21:35:05 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3800071, Evaluate reward: 260.7573852050373\n",
      "\u001b[32m[06-29 21:43:33 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3815737, Evaluate reward: 259.49541998805853\n",
      "\u001b[32m[06-29 21:48:04 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3823482, Evaluate reward: 258.1469794827693\n",
      "\u001b[32m[06-29 21:55:33 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3837443, Evaluate reward: 262.973099266557\n",
      "\u001b[32m[06-29 21:59:55 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3845099, Evaluate reward: 264.49929196722184\n",
      "\u001b[32m[06-29 22:03:45 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3851736, Evaluate reward: 267.27798290357\n",
      "\u001b[32m[06-29 22:11:40 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3865475, Evaluate reward: 265.7996602385317\n",
      "\u001b[32m[06-29 22:15:30 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3871462, Evaluate reward: 264.9981684615084\n",
      "\u001b[32m[06-29 22:23:46 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3885478, Evaluate reward: 266.23395973342224\n",
      "\u001b[32m[06-29 22:28:17 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3893016, Evaluate reward: 269.04852312372566\n",
      "\u001b[32m[06-29 22:35:11 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3905840, Evaluate reward: 268.23590417756265\n",
      "\u001b[32m[06-29 22:38:54 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3912115, Evaluate reward: 267.74518506791384\n",
      "\u001b[32m[06-29 22:45:01 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3923356, Evaluate reward: 262.00370456136517\n",
      "\u001b[32m[06-29 22:52:02 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3936394, Evaluate reward: 261.4587642139978\n",
      "\u001b[32m[06-29 22:56:18 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3943665, Evaluate reward: 258.27256860008026\n",
      "\u001b[32m[06-29 23:00:41 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3951166, Evaluate reward: 259.16881844487847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06-29 23:08:14 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3965272, Evaluate reward: 262.5328613665414\n",
      "\u001b[32m[06-29 23:12:27 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3972570, Evaluate reward: 263.81184196158813\n",
      "\u001b[32m[06-29 23:19:50 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3986418, Evaluate reward: 265.72438235487664\n",
      "\u001b[32m[06-29 23:24:06 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 3993906, Evaluate reward: 267.9550156132989\n",
      "\u001b[32m[06-29 23:30:10 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4005381, Evaluate reward: 272.0535110037259\n",
      "\u001b[32m[06-29 23:34:10 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4012344, Evaluate reward: 263.42604312723626\n",
      "\u001b[32m[06-29 23:40:18 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4023942, Evaluate reward: 270.1162593898314\n",
      "\u001b[32m[06-29 23:43:50 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4030058, Evaluate reward: 267.655414180673\n",
      "\u001b[32m[06-29 23:50:27 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4042606, Evaluate reward: 265.3589879553268\n",
      "\u001b[32m[06-29 23:54:36 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4050068, Evaluate reward: 270.1982437426529\n",
      "\u001b[32m[06-30 00:01:49 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4063516, Evaluate reward: 269.57472373830524\n",
      "\u001b[32m[06-30 00:09:03 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4077379, Evaluate reward: 269.35258263217656\n",
      "\u001b[32m[06-30 00:12:41 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4083649, Evaluate reward: 270.7998884303545\n",
      "\u001b[32m[06-30 00:16:44 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4090811, Evaluate reward: 266.7500052930235\n",
      "\u001b[32m[06-30 00:23:13 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4102909, Evaluate reward: 268.3210139452995\n",
      "\u001b[32m[06-30 00:29:30 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4114689, Evaluate reward: 262.1057504020929\n",
      "\u001b[32m[06-30 00:33:22 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4121357, Evaluate reward: 264.98177304589314\n",
      "\u001b[32m[06-30 00:40:56 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4135784, Evaluate reward: 263.7431191502493\n",
      "\u001b[32m[06-30 00:44:41 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4142267, Evaluate reward: 265.44309622760176\n",
      "\u001b[32m[06-30 00:52:16 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4156808, Evaluate reward: 268.29266862195857\n",
      "\u001b[32m[06-30 00:55:43 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4162739, Evaluate reward: 272.62171355856333\n",
      "\u001b[32m[06-30 01:01:47 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4174114, Evaluate reward: 153.8675629054618\n",
      "\u001b[32m[06-30 01:07:56 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4185660, Evaluate reward: 272.26467060547884\n",
      "\u001b[32m[06-30 01:11:09 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4191077, Evaluate reward: 271.39120695182004\n",
      "\u001b[32m[06-30 01:17:54 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4203793, Evaluate reward: 269.7769867413199\n",
      "\u001b[32m[06-30 01:21:59 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4211138, Evaluate reward: 269.1195870360409\n",
      "\u001b[32m[06-30 01:28:28 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4223468, Evaluate reward: 263.77870009961475\n",
      "\u001b[32m[06-30 01:35:05 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4236090, Evaluate reward: 268.1015708606225\n",
      "\u001b[32m[06-30 01:38:37 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4242227, Evaluate reward: 269.3232228779855\n",
      "\u001b[32m[06-30 01:44:51 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4254103, Evaluate reward: 267.3598458959193\n",
      "\u001b[32m[06-30 01:51:23 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4266883, Evaluate reward: 111.71762110389022\n",
      "\u001b[32m[06-30 01:54:27 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4272115, Evaluate reward: 268.9292487780445\n",
      "\u001b[32m[06-30 02:02:03 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4286967, Evaluate reward: 272.70531683976014\n",
      "\u001b[32m[06-30 02:05:42 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4293413, Evaluate reward: 274.7189143777092\n",
      "\u001b[32m[06-30 02:09:45 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4300727, Evaluate reward: 274.14021035561086\n",
      "\u001b[32m[06-30 02:17:11 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4315256, Evaluate reward: 273.75290191310955\n",
      "\u001b[32m[06-30 02:20:40 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4321288, Evaluate reward: 268.674464427034\n",
      "\u001b[32m[06-30 02:27:34 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4334719, Evaluate reward: 275.0142526326875\n",
      "\u001b[32m[06-30 02:31:19 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4341388, Evaluate reward: 270.7339820949994\n",
      "\u001b[32m[06-30 02:38:38 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4355761, Evaluate reward: 268.88963390960294\n",
      "\u001b[32m[06-30 02:42:38 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4362985, Evaluate reward: 267.6250462238081\n",
      "\u001b[32m[06-30 02:49:18 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4375929, Evaluate reward: 267.45777215228225\n",
      "\u001b[32m[06-30 02:53:33 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4383342, Evaluate reward: 266.42415668658043\n",
      "\u001b[32m[06-30 02:57:52 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4390564, Evaluate reward: 270.72326863114654\n",
      "\u001b[32m[06-30 03:05:58 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4404930, Evaluate reward: 267.37031993345227\n",
      "\u001b[32m[06-30 03:10:16 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4411882, Evaluate reward: 268.56585916873195\n",
      "\u001b[32m[06-30 03:17:09 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4424469, Evaluate reward: 264.0560527318064\n",
      "\u001b[32m[06-30 03:20:52 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4430788, Evaluate reward: 264.2749855889281\n",
      "\u001b[32m[06-30 03:27:48 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4444027, Evaluate reward: 266.14894852150354\n",
      "\u001b[32m[06-30 03:31:39 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4450826, Evaluate reward: 265.24109720557306\n",
      "\u001b[32m[06-30 03:38:39 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4464307, Evaluate reward: 266.91806301270054\n",
      "\u001b[32m[06-30 03:42:43 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4471538, Evaluate reward: 266.4421256596049\n",
      "\u001b[32m[06-30 03:50:08 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4485892, Evaluate reward: 264.4963138538996\n",
      "\u001b[32m[06-30 03:54:13 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4493128, Evaluate reward: 267.3330799802461\n",
      "\u001b[32m[06-30 04:01:23 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4506963, Evaluate reward: 266.0746036556717\n",
      "\u001b[32m[06-30 04:05:28 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4514311, Evaluate reward: 269.41487535476494\n",
      "\u001b[32m[06-30 04:09:14 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4521101, Evaluate reward: 268.99813239207054\n",
      "\u001b[32m[06-30 04:15:56 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4534097, Evaluate reward: 272.06232638291453\n",
      "\u001b[32m[06-30 04:19:21 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4540065, Evaluate reward: 271.5410489319904\n",
      "\u001b[32m[06-30 04:25:05 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4550817, Evaluate reward: 266.4937638800821\n",
      "\u001b[32m[06-30 04:32:32 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4565347, Evaluate reward: 268.04607859765116\n",
      "\u001b[32m[06-30 04:36:27 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4572372, Evaluate reward: 267.64815732521254\n",
      "\u001b[32m[06-30 04:42:10 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4583216, Evaluate reward: 268.3818014512789\n",
      "\u001b[32m[06-30 04:47:07 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4592443, Evaluate reward: 270.72580268063086\n",
      "\u001b[32m[06-30 04:53:34 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4604853, Evaluate reward: 267.161354824697\n",
      "\u001b[32m[06-30 04:57:31 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4611986, Evaluate reward: 270.09648205053065\n",
      "\u001b[32m[06-30 05:03:57 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4624398, Evaluate reward: 269.33047166732695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06-30 05:09:20 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4634609, Evaluate reward: 272.9855363217468\n",
      "\u001b[32m[06-30 05:13:12 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4641587, Evaluate reward: 269.59091266858263\n",
      "\u001b[32m[06-30 05:20:24 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4655600, Evaluate reward: 271.0869479456587\n",
      "\u001b[32m[06-30 05:24:11 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4662436, Evaluate reward: 272.5511168181399\n",
      "\u001b[32m[06-30 05:30:59 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4675881, Evaluate reward: 96.56334555957963\n",
      "\u001b[32m[06-30 05:33:41 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4680400, Evaluate reward: 272.59603741568884\n",
      "\u001b[32m[06-30 05:40:49 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4694634, Evaluate reward: 73.53664788946944\n",
      "\u001b[32m[06-30 05:44:39 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4701541, Evaluate reward: 270.8883795129005\n",
      "\u001b[32m[06-30 05:50:03 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4711764, Evaluate reward: 270.5055554504935\n",
      "\u001b[32m[06-30 05:56:07 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4723297, Evaluate reward: 272.60691124868947\n",
      "\u001b[32m[06-30 06:02:39 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4735995, Evaluate reward: 270.55184595321987\n",
      "\u001b[32m[06-30 06:05:41 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4741097, Evaluate reward: 266.1549834094519\n",
      "\u001b[32m[06-30 06:12:02 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4753494, Evaluate reward: 70.07013009293192\n",
      "\u001b[32m[06-30 06:18:01 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4765063, Evaluate reward: 142.88605418016326\n",
      "\u001b[32m[06-30 06:21:41 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4771634, Evaluate reward: 264.41662611848363\n",
      "\u001b[32m[06-30 06:28:30 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4784835, Evaluate reward: 262.21235367497115\n",
      "\u001b[32m[06-30 06:32:07 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4791245, Evaluate reward: 262.59803218226875\n",
      "\u001b[32m[06-30 06:40:08 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4805560, Evaluate reward: 263.26360011710665\n",
      "\u001b[32m[06-30 06:44:08 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4812832, Evaluate reward: 266.27618339175547\n",
      "\u001b[32m[06-30 06:48:04 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4820100, Evaluate reward: 265.8685516904412\n",
      "\u001b[32m[06-30 06:55:32 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4834379, Evaluate reward: 265.25592891928926\n",
      "\u001b[32m[06-30 06:59:36 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4841401, Evaluate reward: 266.69774173298356\n",
      "\u001b[32m[06-30 07:07:25 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4855552, Evaluate reward: 271.9691546775929\n",
      "\u001b[32m[06-30 07:11:03 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4861806, Evaluate reward: 271.23160097854463\n",
      "\u001b[32m[06-30 07:16:25 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4872241, Evaluate reward: 269.73632973235283\n",
      "\u001b[32m[06-30 07:23:09 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4885725, Evaluate reward: 122.7814539220395\n",
      "\u001b[32m[06-30 07:26:51 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4892622, Evaluate reward: 269.5314259733924\n",
      "\u001b[32m[06-30 07:32:47 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4904370, Evaluate reward: 268.5264275310611\n",
      "\u001b[32m[06-30 07:38:16 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4915327, Evaluate reward: 274.6001430580312\n",
      "\u001b[32m[06-30 07:41:35 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4921555, Evaluate reward: 270.6418362500903\n",
      "\u001b[32m[06-30 07:47:45 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4934209, Evaluate reward: 269.367898975602\n",
      "\u001b[32m[06-30 07:52:43 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4944104, Evaluate reward: 265.28498377225236\n",
      "\u001b[32m[06-30 07:57:32 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4953824, Evaluate reward: 269.54050186029326\n",
      "\u001b[32m[06-30 08:03:30 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4966153, Evaluate reward: 268.36029550388645\n",
      "\u001b[32m[06-30 08:06:11 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4971016, Evaluate reward: 267.3806160817792\n",
      "\u001b[32m[06-30 08:12:39 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4984461, Evaluate reward: 267.36134840530406\n",
      "\u001b[32m[06-30 08:18:36 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 4996468, Evaluate reward: 268.557027635891\n",
      "\u001b[32m[06-30 08:22:02 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 5002983, Evaluate reward: 265.572889650148\n",
      "\u001b[32m[06-30 08:28:06 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 5015412, Evaluate reward: 266.60545228933483\n",
      "\u001b[32m[06-30 08:31:17 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 5021289, Evaluate reward: 265.5367433881896\n",
      "\u001b[32m[06-30 08:36:55 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 5032649, Evaluate reward: 267.6287524398329\n",
      "\u001b[32m[06-30 08:43:22 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 5045922, Evaluate reward: 267.22996526854485\n",
      "\u001b[32m[06-30 08:46:29 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 5051724, Evaluate reward: 266.6165689810341\n",
      "\u001b[32m[06-30 08:52:32 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 5064153, Evaluate reward: 266.5955792689621\n",
      "\u001b[32m[06-30 08:55:57 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 5070541, Evaluate reward: 265.92647383495046\n",
      "\u001b[32m[06-30 09:01:50 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 5082330, Evaluate reward: 264.5687181616804\n",
      "\u001b[32m[06-30 09:07:55 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 5094674, Evaluate reward: 261.73276283312526\n",
      "\u001b[32m[06-30 09:11:15 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 5100899, Evaluate reward: 263.46577050030976\n",
      "\u001b[32m[06-30 09:17:58 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 5114682, Evaluate reward: 264.46001495893734\n",
      "\u001b[32m[06-30 09:21:39 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 5121713, Evaluate reward: 266.80587274790685\n",
      "\u001b[32m[06-30 09:27:09 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 5132818, Evaluate reward: 270.7454433989841\n",
      "\u001b[32m[06-30 09:33:17 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 5145304, Evaluate reward: 273.40722707141316\n",
      "\u001b[32m[06-30 09:37:21 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 5150969, Evaluate reward: 269.89904938886934\n",
      "\u001b[32m[06-30 09:44:50 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 5162930, Evaluate reward: 265.08436409803176\n",
      "\u001b[32m[06-30 09:52:12 MainThread @<ipython-input-11-8251e3445aae>:51]\u001b[0m Steps 5175309, Evaluate reward: 267.33235991966967\n"
     ]
    }
   ],
   "source": [
    "# 创建飞行器环境\n",
    "import gym\n",
    "env = gym.make(\"BipedalWalker-v2\")\n",
    "env.reset()\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "obs_dim += 1  # add 1 to obs dim for time step feature\n",
    "\n",
    "scaler = Scaler(obs_dim)\n",
    "\n",
    "\n",
    "\n",
    "# 根据parl框架构建agent\n",
    "######################################################################\n",
    "######################################################################\n",
    "#\n",
    "# 6. 请构建agent:  QuadrotorModel, DDPG, QuadrotorAgent三者嵌套\n",
    "#\n",
    "######################################################################\n",
    "######################################################################\n",
    "\n",
    "model = BipedalWalkerModel(obs_dim, act_dim, -1.0)\n",
    "alg = PPO(model, act_dim=act_dim,policy_lr=ACTOR_LR,value_lr=CRITIC_LR)\n",
    "agent = BiPedalwalkerAgent(alg, obs_dim, act_dim, 0.003, loss_type='CLIP')\n",
    "\n",
    "\n",
    "# parl库也为DDPG算法内置了ReplayMemory，可直接从 parl.utils 引入使用\n",
    "collect_trajectories(env, agent, scaler, episodes=5)\n",
    "\n",
    "test_flag = 0\n",
    "total_steps = 0\n",
    "while total_steps < 100000000:\n",
    "    trajectories = collect_trajectories(\n",
    "        env, agent, scaler, episodes=5)\n",
    "    total_steps += sum([t['obs'].shape[0] for t in trajectories])\n",
    "    total_train_rewards = sum([np.sum(t['rewards']) for t in trajectories])\n",
    "\n",
    "    train_obs, train_actions, train_advantages, train_discount_sum_rewards = build_train_data(\n",
    "        trajectories, agent)\n",
    "\n",
    "    policy_loss, kl = agent.policy_learn(train_obs, train_actions,\n",
    "                                         train_advantages)\n",
    "    value_loss = agent.value_learn(train_obs, train_discount_sum_rewards)\n",
    "\n",
    "    if total_steps // TEST_EVERY_STEPS >= test_flag: # 每隔一定step数，评估一次模型\n",
    "        while total_steps // TEST_EVERY_STEPS >= test_flag:\n",
    "            test_flag += 1\n",
    " \n",
    "        eval_reward = run_evaluate_episode(env, agent, scaler)\n",
    "        logger.info('Steps {}, Evaluate reward: {}'.format(\n",
    "                total_steps, eval_reward))# 打印评估的reward\n",
    "\n",
    "        # 每评估一次，就保存一次模型，以训练的step数命名\n",
    "#         ckpt = 'model_dir/steps_{}.ckpt'.format(total_steps)\n",
    "#         agent.save(ckpt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
